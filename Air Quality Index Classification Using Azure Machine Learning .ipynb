{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Air Quality Index Classification Using Azure Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Azure SDK Installation\n",
    "This section installs the necessary Azure SDK libraries, such as `azure-ai-ml` and other required dependencies for integrating with Azure Machine Learning services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "gather": {
     "logged": 1727419407134
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: azure-ai-ml in /anaconda/envs/azureml_py38/lib/python3.9/site-packages (1.20.0)\n",
      "Requirement already satisfied: opencensus-ext-logging in /anaconda/envs/azureml_py38/lib/python3.9/site-packages (from azure-ai-ml) (0.1.1)\n",
      "Requirement already satisfied: pydash>=6.0.0 in /anaconda/envs/azureml_py38/lib/python3.9/site-packages (from azure-ai-ml) (8.0.3)\n",
      "Requirement already satisfied: tqdm in /anaconda/envs/azureml_py38/lib/python3.9/site-packages (from azure-ai-ml) (4.66.2)\n",
      "Requirement already satisfied: isodate in /anaconda/envs/azureml_py38/lib/python3.9/site-packages (from azure-ai-ml) (0.6.1)\n",
      "Requirement already satisfied: azure-storage-file-share in /anaconda/envs/azureml_py38/lib/python3.9/site-packages (from azure-ai-ml) (12.18.0)\n",
      "Requirement already satisfied: azure-core>=1.23.0 in /anaconda/envs/azureml_py38/lib/python3.9/site-packages (from azure-ai-ml) (1.30.1)\n",
      "Requirement already satisfied: marshmallow>=3.5 in /anaconda/envs/azureml_py38/lib/python3.9/site-packages (from azure-ai-ml) (3.22.0)\n",
      "Requirement already satisfied: pyjwt in /anaconda/envs/azureml_py38/lib/python3.9/site-packages (from azure-ai-ml) (2.4.0)\n",
      "Requirement already satisfied: colorama in /anaconda/envs/azureml_py38/lib/python3.9/site-packages (from azure-ai-ml) (0.4.6)\n",
      "Requirement already satisfied: azure-mgmt-core>=1.3.0 in /anaconda/envs/azureml_py38/lib/python3.9/site-packages (from azure-ai-ml) (1.4.0)\n",
      "Requirement already satisfied: jsonschema>=4.0.0 in /anaconda/envs/azureml_py38/lib/python3.9/site-packages (from azure-ai-ml) (4.21.1)\n",
      "Requirement already satisfied: msrest>=0.6.18 in /anaconda/envs/azureml_py38/lib/python3.9/site-packages (from azure-ai-ml) (0.7.1)\n",
      "Requirement already satisfied: typing-extensions in /anaconda/envs/azureml_py38/lib/python3.9/site-packages (from azure-ai-ml) (4.12.2)\n",
      "Requirement already satisfied: opencensus-ext-azure in /anaconda/envs/azureml_py38/lib/python3.9/site-packages (from azure-ai-ml) (1.1.13)\n",
      "Requirement already satisfied: azure-storage-file-datalake>=12.2.0 in /anaconda/envs/azureml_py38/lib/python3.9/site-packages (from azure-ai-ml) (12.17.0)\n",
      "Requirement already satisfied: azure-common>=1.1 in /anaconda/envs/azureml_py38/lib/python3.9/site-packages (from azure-ai-ml) (1.1.28)\n",
      "Requirement already satisfied: pyyaml>=5.1.0 in /anaconda/envs/azureml_py38/lib/python3.9/site-packages (from azure-ai-ml) (6.0.1)\n",
      "Requirement already satisfied: strictyaml in /anaconda/envs/azureml_py38/lib/python3.9/site-packages (from azure-ai-ml) (1.7.3)\n",
      "Requirement already satisfied: azure-storage-blob>=12.10.0 in /anaconda/envs/azureml_py38/lib/python3.9/site-packages (from azure-ai-ml) (12.23.1)\n",
      "Requirement already satisfied: requests>=2.21.0 in /anaconda/envs/azureml_py38/lib/python3.9/site-packages (from azure-core>=1.23.0->azure-ai-ml) (2.31.0)\n",
      "Requirement already satisfied: six>=1.11.0 in /anaconda/envs/azureml_py38/lib/python3.9/site-packages (from azure-core>=1.23.0->azure-ai-ml) (1.16.0)\n",
      "Requirement already satisfied: cryptography>=2.1.4 in /anaconda/envs/azureml_py38/lib/python3.9/site-packages (from azure-storage-blob>=12.10.0->azure-ai-ml) (38.0.4)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /anaconda/envs/azureml_py38/lib/python3.9/site-packages (from jsonschema>=4.0.0->azure-ai-ml) (0.18.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /anaconda/envs/azureml_py38/lib/python3.9/site-packages (from jsonschema>=4.0.0->azure-ai-ml) (2023.12.1)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /anaconda/envs/azureml_py38/lib/python3.9/site-packages (from jsonschema>=4.0.0->azure-ai-ml) (23.2.0)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /anaconda/envs/azureml_py38/lib/python3.9/site-packages (from jsonschema>=4.0.0->azure-ai-ml) (0.35.0)\n",
      "Requirement already satisfied: packaging>=17.0 in /anaconda/envs/azureml_py38/lib/python3.9/site-packages (from marshmallow>=3.5->azure-ai-ml) (24.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /anaconda/envs/azureml_py38/lib/python3.9/site-packages (from msrest>=0.6.18->azure-ai-ml) (2022.9.24)\n",
      "Requirement already satisfied: requests-oauthlib>=0.5.0 in /anaconda/envs/azureml_py38/lib/python3.9/site-packages (from msrest>=0.6.18->azure-ai-ml) (2.0.0)\n",
      "Requirement already satisfied: opencensus<1.0.0,>=0.11.4 in /anaconda/envs/azureml_py38/lib/python3.9/site-packages (from opencensus-ext-azure->azure-ai-ml) (0.11.4)\n",
      "Requirement already satisfied: psutil>=5.6.3 in /anaconda/envs/azureml_py38/lib/python3.9/site-packages (from opencensus-ext-azure->azure-ai-ml) (5.9.3)\n",
      "Requirement already satisfied: azure-identity<2.0.0,>=1.5.0 in /anaconda/envs/azureml_py38/lib/python3.9/site-packages (from opencensus-ext-azure->azure-ai-ml) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.6.0 in /anaconda/envs/azureml_py38/lib/python3.9/site-packages (from strictyaml->azure-ai-ml) (2.9.0.post0)\n",
      "Requirement already satisfied: msal>=1.24.0 in /anaconda/envs/azureml_py38/lib/python3.9/site-packages (from azure-identity<2.0.0,>=1.5.0->opencensus-ext-azure->azure-ai-ml) (1.27.0)\n",
      "Requirement already satisfied: msal-extensions>=0.3.0 in /anaconda/envs/azureml_py38/lib/python3.9/site-packages (from azure-identity<2.0.0,>=1.5.0->opencensus-ext-azure->azure-ai-ml) (1.0.0)\n",
      "Requirement already satisfied: cffi>=1.12 in /anaconda/envs/azureml_py38/lib/python3.9/site-packages (from cryptography>=2.1.4->azure-storage-blob>=12.10.0->azure-ai-ml) (1.16.0)\n",
      "Requirement already satisfied: google-api-core<3.0.0,>=1.0.0 in /anaconda/envs/azureml_py38/lib/python3.9/site-packages (from opencensus<1.0.0,>=0.11.4->opencensus-ext-azure->azure-ai-ml) (2.18.0)\n",
      "Requirement already satisfied: opencensus-context>=0.1.3 in /anaconda/envs/azureml_py38/lib/python3.9/site-packages (from opencensus<1.0.0,>=0.11.4->opencensus-ext-azure->azure-ai-ml) (0.1.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /anaconda/envs/azureml_py38/lib/python3.9/site-packages (from requests>=2.21.0->azure-core>=1.23.0->azure-ai-ml) (3.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /anaconda/envs/azureml_py38/lib/python3.9/site-packages (from requests>=2.21.0->azure-core>=1.23.0->azure-ai-ml) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /anaconda/envs/azureml_py38/lib/python3.9/site-packages (from requests>=2.21.0->azure-core>=1.23.0->azure-ai-ml) (1.26.18)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /anaconda/envs/azureml_py38/lib/python3.9/site-packages (from requests-oauthlib>=0.5.0->msrest>=0.6.18->azure-ai-ml) (3.2.2)\n",
      "Requirement already satisfied: pycparser in /anaconda/envs/azureml_py38/lib/python3.9/site-packages (from cffi>=1.12->cryptography>=2.1.4->azure-storage-blob>=12.10.0->azure-ai-ml) (2.22)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5 in /anaconda/envs/azureml_py38/lib/python3.9/site-packages (from google-api-core<3.0.0,>=1.0.0->opencensus<1.0.0,>=0.11.4->opencensus-ext-azure->azure-ai-ml) (3.20.3)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /anaconda/envs/azureml_py38/lib/python3.9/site-packages (from google-api-core<3.0.0,>=1.0.0->opencensus<1.0.0,>=0.11.4->opencensus-ext-azure->azure-ai-ml) (1.23.0)\n",
      "Requirement already satisfied: google-auth<3.0.dev0,>=2.14.1 in /anaconda/envs/azureml_py38/lib/python3.9/site-packages (from google-api-core<3.0.0,>=1.0.0->opencensus<1.0.0,>=0.11.4->opencensus-ext-azure->azure-ai-ml) (2.29.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /anaconda/envs/azureml_py38/lib/python3.9/site-packages (from google-api-core<3.0.0,>=1.0.0->opencensus<1.0.0,>=0.11.4->opencensus-ext-azure->azure-ai-ml) (1.63.0)\n",
      "Requirement already satisfied: portalocker<3,>=1.0 in /anaconda/envs/azureml_py38/lib/python3.9/site-packages (from msal-extensions>=0.3.0->azure-identity<2.0.0,>=1.5.0->opencensus-ext-azure->azure-ai-ml) (2.8.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /anaconda/envs/azureml_py38/lib/python3.9/site-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus<1.0.0,>=0.11.4->opencensus-ext-azure->azure-ai-ml) (4.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /anaconda/envs/azureml_py38/lib/python3.9/site-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus<1.0.0,>=0.11.4->opencensus-ext-azure->azure-ai-ml) (0.4.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /anaconda/envs/azureml_py38/lib/python3.9/site-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus<1.0.0,>=0.11.4->opencensus-ext-azure->azure-ai-ml) (5.3.3)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /anaconda/envs/azureml_py38/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus<1.0.0,>=0.11.4->opencensus-ext-azure->azure-ai-ml) (0.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install azure-ai-ml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Azure Credential Handling\n",
    "Here, the notebook uses `DefaultAzureCredential` to handle authentication, with a fallback to `InteractiveBrowserCredential` in case the default method fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1727419410086
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
    "from azure.ai.ml import MLClient\n",
    "\n",
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    # Check if given credential can get token successfully.\n",
    "    credential.get_token(\"https://management.azure.com/.default\")\n",
    "except Exception as ex:\n",
    "    # Fall back to InteractiveBrowserCredential in case DefaultAzureCredential not work\n",
    "    credential = InteractiveBrowserCredential()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workspace Setup\n",
    "This section connects to the Azure Machine Learning workspace using `MLClient.from_config()`, which reads workspace configuration from a `config.json` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1727419410465
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found the config file in: /config.json\n"
     ]
    }
   ],
   "source": [
    "# Get a handle to workspace\n",
    "ml_client = MLClient.from_config(credential=credential)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Environment Setup\n",
    "A custom environment is defined using an `environment.yml` file. This file specifies the Python version, and includes common libraries like `pandas`, `numpy`, `scikit-learn`, `xgboost`, and others. It also ensures necessary libraries like `imbalanced-learn` are available for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1726740255902
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting environment.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile environment.yml\n",
    "name: custom-azureml-sklearn-env\n",
    "channels:\n",
    "  - defaults\n",
    "  - conda-forge\n",
    "dependencies:\n",
    "  - python=3.8\n",
    "  - pandas\n",
    "  - numpy\n",
    "  - scikit-learn=0.24\n",
    "  - catboost\n",
    "  - lightgbm\n",
    "  - xgboost\n",
    "  - pip\n",
    "  - pip:\n",
    "      - mlflow\n",
    "      - imbalanced-learn  # Ensure this line is present\n",
    "      - argparse\n",
    "      - matplotlib\n",
    "      - seaborn\n",
    "      - azureml-sdk\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1727419411939
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Environment({'arm_type': 'environment_version', 'latest_version': None, 'image': 'mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04:latest', 'intellectual_property': None, 'is_anonymous': False, 'auto_increment_version': False, 'auto_delete_setting': None, 'name': 'custom-azureml-sklearn-env', 'description': None, 'tags': {}, 'properties': {'azureml.labels': 'latest'}, 'print_as_yaml': False, 'id': '/subscriptions/edc61183-6780-4b6c-8c4a-a5fb6ae6b788/resourceGroups/raghavavigneshwar/providers/Microsoft.MachineLearningServices/workspaces/raghava/environments/custom-azureml-sklearn-env/versions/5', 'Resource__source_path': '', 'base_path': '/mnt/batch/tasks/shared/LS_root/mounts/clusters/raghava/code/Users/User1-44320065', 'creation_context': <azure.ai.ml.entities._system_data.SystemData object at 0x7f0a4c4d7a00>, 'serialize': <msrest.serialization.Serializer object at 0x7f0a4c4e2dc0>, 'version': '5', 'conda_file': {'channels': ['defaults', 'conda-forge'], 'dependencies': ['python=3.8', 'pandas', 'numpy', 'scikit-learn=0.24', 'catboost', 'lightgbm', 'xgboost', 'pip', {'pip': ['mlflow', 'imbalanced-learn', 'argparse', 'matplotlib', 'seaborn', 'azureml-sdk']}], 'name': 'custom-azureml-sklearn-env'}, 'build': None, 'inference_config': None, 'os_type': 'Linux', 'conda_file_path': None, 'path': None, 'datastore': None, 'upload_hash': None, 'translated_conda_file': '{\\n  \"channels\": [\\n    \"defaults\",\\n    \"conda-forge\"\\n  ],\\n  \"dependencies\": [\\n    \"python=3.8\",\\n    \"pandas\",\\n    \"numpy\",\\n    \"scikit-learn=0.24\",\\n    \"catboost\",\\n    \"lightgbm\",\\n    \"xgboost\",\\n    \"pip\",\\n    {\\n      \"pip\": [\\n        \"mlflow\",\\n        \"imbalanced-learn\",\\n        \"argparse\",\\n        \"matplotlib\",\\n        \"seaborn\",\\n        \"azureml-sdk\"\\n      ]\\n    }\\n  ],\\n  \"name\": \"custom-azureml-sklearn-env\"\\n}'})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azure.ai.ml.entities import Environment\n",
    "from azure.ai.ml import MLClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "# Load environment from the conda YAML file\n",
    "env = Environment(\n",
    "    name=\"custom-azureml-sklearn-env\",\n",
    "    conda_file=\"environment.yml\",\n",
    "    image=\"mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04:latest\"\n",
    ")\n",
    "\n",
    "# Register the environment in Azure ML workspace\n",
    "ml_client.environments.create_or_update(env)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Scripts and JSON Files\n",
    "In this project, Python scripts are used to define various components of the machine learning workflow, such as data preparation, model training, and evaluation. These scripts are often parameterized to make them reusable and flexible.\n",
    "\n",
    "The `config.json` file is used to store workspace configuration information, such as the subscription ID, resource group, and workspace name. It allows for seamless connection to the Azure Machine Learning workspace without manually specifying these details in every script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1727419412227
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src folder created\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# create a folder for the script files\n",
    "script_folder = 'src'\n",
    "os.makedirs(script_folder, exist_ok=True)\n",
    "print(script_folder, 'folder created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/load_data.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $script_folder/load_data.py\n",
    "import argparse\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def main(args):\n",
    "    df = get_data(args.input_data)\n",
    "    df.to_csv((Path(args.output_data) / \"loaded_data.csv\"), index=False)\n",
    "\n",
    "def get_data(path):\n",
    "    df = pd.read_csv(path)\n",
    "    row_count = len(df)\n",
    "    print('Preparing {} rows of data'.format(row_count))\n",
    "    return df\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--input_data\", dest='input_data', type=str)\n",
    "    parser.add_argument(\"--output_data\", dest='output_data', type=str)\n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    args = parse_args()\n",
    "    main(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/clean_data.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $script_folder/clean_data.py\n",
    "import argparse\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "import pickle\n",
    "import json\n",
    "import glob\n",
    "\n",
    "def main(args):\n",
    "    df = get_data(args.input_data)\n",
    "    cleaned_data = clean_data(df)\n",
    "    cleaned_data.to_csv((Path(args.output_data) / \"cleaned_data.csv\"), index=False)\n",
    "    \n",
    "def get_data(data_path):\n",
    "    all_files = glob.glob(data_path + \"/*.csv\")\n",
    "    df = pd.concat((pd.read_csv(f) for f in all_files), sort=False)\n",
    "    return df\n",
    "\n",
    "def clean_data(df):\n",
    "    df = df.dropna(subset=['AQI'])\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    print(\"Removed rows with missing 'AQI' values. Remaining rows: {}\".format(len(df)))\n",
    "    return df\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--input_data\", dest='input_data', type=str)\n",
    "    parser.add_argument(\"--output_data\", dest='output_data', type=str)\n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    args = parse_args()\n",
    "    main(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/impute_data.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $script_folder/impute_data.py\n",
    "import argparse\n",
    "import pandas as pd\n",
    "from sklearn.impute import KNNImputer\n",
    "from pathlib import Path\n",
    "import os\n",
    "import pickle\n",
    "import json\n",
    "import glob\n",
    "\n",
    "def main(args):\n",
    "    df = get_data(args.input_data)\n",
    "    imputed_data = impute_missing_values(df)\n",
    "    imputed_data.to_csv((Path(args.output_data) / \"imputed_data.csv\"), index=False)\n",
    "    \n",
    "def get_data(data_path):\n",
    "    all_files = glob.glob(data_path + \"/*.csv\")\n",
    "    df = pd.concat((pd.read_csv(f) for f in all_files), sort=False)\n",
    "    return df\n",
    "\n",
    "def impute_missing_values(df):\n",
    "    missing_features = ['PM2.5', 'PM10', 'NO', 'NO2', 'NOx', 'NH3', 'CO', 'SO2', 'O3', 'Benzene', 'Toluene', 'Xylene']\n",
    "    for feature in missing_features:\n",
    "        if df[feature].isnull().sum() == 0:\n",
    "            continue\n",
    "        \n",
    "        non_missing_indices = ~df['AQI'].isnull() & ~df[feature].isnull()\n",
    "        X = df.loc[non_missing_indices, ['AQI']]\n",
    "        y = df.loc[non_missing_indices, feature]\n",
    "        \n",
    "        imputer = KNNImputer(n_neighbors=5)\n",
    "        imputer.fit(X, y)\n",
    "        \n",
    "        missing_indices = df[feature].isnull()\n",
    "        X_missing = df.loc[missing_indices, ['AQI']]\n",
    "        \n",
    "        if not X_missing.empty:\n",
    "            imputed_values = imputer.transform(X_missing)\n",
    "            df.loc[missing_indices, feature] = imputed_values\n",
    "        \n",
    "        print(f\"Imputed missing values for feature: {feature}\")\n",
    "    return df\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--input_data\", dest='input_data', type=str)\n",
    "    parser.add_argument(\"--output_data\", dest='output_data', type=str)\n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    args = parse_args()\n",
    "    main(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/handle_outliers.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $script_folder/handle_outliers.py\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle\n",
    "import json\n",
    "import glob\n",
    "\n",
    "def main(args):\n",
    "    df = get_data(args.input_data)\n",
    "    visualize_outliers(df, args.output_data)\n",
    "    print(\"Outlier visualization complete.\")\n",
    "    final_cleaned_data = replace_outliers(df)\n",
    "    final_cleaned_data.to_csv((Path(args.output_data) / \"final_cleaned_data.csv\"), index=False)\n",
    "\n",
    "def get_data(data_path):\n",
    "    all_files = glob.glob(data_path + \"/*.csv\")\n",
    "    df = pd.concat((pd.read_csv(f) for f in all_files), sort=False)\n",
    "    return df\n",
    "\n",
    "def detect_outliers_zscore(data):\n",
    "    outliers = []\n",
    "    threshold = 3\n",
    "    mean = np.mean(data)\n",
    "    std = np.std(data)\n",
    "    for value in data:\n",
    "        z_score = (value - mean) / std\n",
    "        if np.abs(z_score) > threshold:\n",
    "            outliers.append(value)\n",
    "    return outliers\n",
    "\n",
    "def replace_outliers_with_max(data, outliers):\n",
    "    Q3 = np.percentile(data, 75)\n",
    "    max_within_Q3 = np.max(data[data <= Q3])\n",
    "    data[data.isin(outliers)] = max_within_Q3\n",
    "\n",
    "def replace_outliers(df):\n",
    "    numerical_features = ['PM2.5', 'PM10', 'NO', 'NO2', 'NOx', 'NH3', 'CO', 'SO2', 'O3', 'Benzene', 'Toluene', 'Xylene', 'AQI']\n",
    "    cleaned_df = df.copy()\n",
    "    for feature in numerical_features:\n",
    "        outliers = detect_outliers_zscore(cleaned_df[feature])\n",
    "        if len(outliers) > 0:\n",
    "            replace_outliers_with_max(cleaned_df[feature], outliers)\n",
    "    cleaned_df.reset_index(drop=True, inplace=True)\n",
    "    print(\"Outliers have been detected and replaced.\")\n",
    "    return cleaned_df\n",
    "\n",
    "def visualize_outliers(df, output_data_path):\n",
    "    numerical_features = ['PM2.5', 'PM10', 'NO', 'NO2', 'NOx', 'NH3', 'CO', 'SO2', 'O3', 'Benzene', 'Toluene', 'Xylene', 'AQI']\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.boxplot(data=df[numerical_features])\n",
    "    plt.xlabel('Features')\n",
    "    plt.ylabel('Values')\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "    # Save the figure as an image file\n",
    "    plt.savefig(Path(output_data_path) / \"outlier_visualization.png\")\n",
    "    plt.close()  # Close the figure to free up memory\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--input_data\", dest='input_data', type=str)\n",
    "    parser.add_argument(\"--output_data\", dest='output_data', type=str)\n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    args = parse_args()\n",
    "    main(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/train_test_split_component.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $script_folder/train_test_split_component.py\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    df = get_data(args.input_data)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = split_and_transform_data(df)\n",
    "    \n",
    "    # Save the data\n",
    "    X_train.to_csv(Path(args.X_train_data) , index=False)\n",
    "    X_test.to_csv(Path(args.X_test_data), index=False)\n",
    "    y_train.to_csv(Path(args.y_train_data) , index=False)\n",
    "    y_test.to_csv(Path(args.y_test_data), index=False)\n",
    "\n",
    "\n",
    "def get_data(data_path):\n",
    "    all_files = glob.glob(data_path + \"/*.csv\")\n",
    "    df = pd.concat((pd.read_csv(f) for f in all_files), sort=False)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def split_and_transform_data(df):\n",
    "    # Defining features and target\n",
    "    features = ['PM2.5', 'PM10', 'NO', 'NO2', 'NOx', 'NH3', 'CO', 'SO2', 'O3', 'Benzene', 'Toluene', 'Xylene']\n",
    "    target = 'AQI_Bucket'\n",
    "\n",
    "    # Splitting features and target\n",
    "    X = df[features]\n",
    "    y = df[target]\n",
    "\n",
    "    # Split into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "    # Applying standard scaling to the features\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    # Applying LabelEncoder to the target variable\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_train = label_encoder.fit_transform(y_train)\n",
    "    y_test = label_encoder.transform(y_test)\n",
    "\n",
    "    # Convert back to DataFrame for saving\n",
    "    X_train = pd.DataFrame(X_train, columns=features)\n",
    "    X_test = pd.DataFrame(X_test, columns=features)\n",
    "    y_train = pd.DataFrame(y_train, columns=[target])\n",
    "    y_test = pd.DataFrame(y_test, columns=[target])\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--input_data\", type=str, required=True, help=\"Path to input data\")\n",
    "    parser.add_argument(\"--X_train_data\", type=str, required=True, help=\"Path to save training data\")\n",
    "    parser.add_argument(\"--X_test_data\", type=str, required=True, help=\"Path to save testing data\")\n",
    "    parser.add_argument(\"--y_train_data\", type=str, required=True, help=\"Path to save training data\")\n",
    "    parser.add_argument(\"--y_test_data\", type=str, required=True, help=\"Path to save testing data\")\n",
    "    return parser.parse_args()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    args = parse_args()\n",
    "    main(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/apply_smote.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $script_folder/apply_smote.py\n",
    "import argparse\n",
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    # Load the training data\n",
    "    X_train = pd.read_csv(args.X_train_data)\n",
    "    y_train = pd.read_csv(args.y_train_data)\n",
    "\n",
    "    # Perform SMOTE to handle imbalanced data\n",
    "    X_resampled, y_resampled = apply_smote(X_train, y_train)\n",
    "\n",
    "    # Save the resampled data\n",
    "    X_resampled.to_csv(Path(args.X_resampled_data), index=False)\n",
    "    y_resampled.to_csv(Path(args.y_resampled_data), index=False)\n",
    "\n",
    "def get_data(data_path):\n",
    "    all_files = glob.glob(data_path + \"/*.csv\")\n",
    "    df = pd.concat((pd.read_csv(f) for f in all_files), sort=False)\n",
    "    return df\n",
    "\n",
    "def apply_smote(X, y):\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_resampled, y_resampled = smote.fit_resample(X, y.values.ravel())  # Ensure y is the correct shape\n",
    "\n",
    "    # Convert back to DataFrame for saving\n",
    "    X_resampled = pd.DataFrame(X_resampled, columns=X.columns)\n",
    "    y_resampled = pd.DataFrame(y_resampled, columns=y.columns)\n",
    "\n",
    "    # Print the lengths of the resampled data\n",
    "    print(f\"Length of X_resampled: {len(X_resampled)}\")\n",
    "    print(f\"Length of y_resampled: {len(y_resampled)}\")\n",
    "\n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--X_train_data\", type=str, required=True, help=\"Path to training data\")\n",
    "    parser.add_argument(\"--y_train_data\", type=str, required=True, help=\"Path to training labels\")\n",
    "    parser.add_argument(\"--X_resampled_data\", type=str, required=True, help=\"Path to save resampled training data\")\n",
    "    parser.add_argument(\"--y_resampled_data\", type=str, required=True, help=\"Path to save resampled training labels\")\n",
    "    return parser.parse_args()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    args = parse_args()\n",
    "    main(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/modeling_component.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $script_folder/modeling_component.py\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import joblib  # Import joblib to save the model\n",
    "from azureml.core.run import Run  # Import Azure ML Run\n",
    "\n",
    "def main(args):\n",
    "    run = Run.get_context()  # Get the Azure ML run context\n",
    "\n",
    "    # Load the data\n",
    "    X_train = pd.read_csv(args.X_train_resampled)\n",
    "    y_train = pd.read_csv(args.y_train_resampled).values.ravel()  # Flattening the array\n",
    "    X_test = pd.read_csv(args.X_test)\n",
    "    X_train1 = pd.read_csv(args.X_train)\n",
    "\n",
    "    # Initialize the XGBoost classifier\n",
    "    model = xgb.XGBClassifier(use_label_encoder=False, eval_metric=\"mlogloss\", random_state=42)\n",
    "\n",
    "    # Define the parameter grid for Grid Search\n",
    "    param_grid = {\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'n_estimators': [50, 100, 150],\n",
    "        'subsample': [0.5, 0.7, 1.0]\n",
    "    }\n",
    "\n",
    "    # Perform Grid Search with cross-validation\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, scoring='f1', cv=3, verbose=1)\n",
    "\n",
    "    # Train the model\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Get the best parameters and score\n",
    "    best_params = grid_search.best_params_\n",
    "    best_f1_score = grid_search.best_score_\n",
    "    print(\"Best Parameters:\", best_params)\n",
    "    print(\"Best F1 Score:\", best_f1_score)\n",
    "\n",
    "    # Log parameters and metrics to Azure ML\n",
    "    run.log(\"Best F1 Score\", best_f1_score)\n",
    "    for param, value in best_params.items():\n",
    "        run.log(param, value)\n",
    "\n",
    "    # Save the best model to a file\n",
    "    best_model = grid_search.best_estimator_\n",
    "    joblib.dump(best_model, \"best_model.pkl\")\n",
    "\n",
    "    # Upload the model to the run's outputs\n",
    "    run.upload_file(name='outputs/best_model.pkl', path_or_stream='best_model.pkl')\n",
    "\n",
    "    # Make predictions\n",
    "    y_train_pred1 = grid_search.predict(X_train)\n",
    "    y_train_pred = grid_search.predict(X_train1)\n",
    "    y_test_pred = grid_search.predict(X_test)\n",
    "\n",
    "    # Calculate F1 score for training predictions\n",
    "    train_f1 = f1_score(y_train, y_train_pred1, average='weighted')\n",
    "    print(\"Train F1 Score:\", train_f1)\n",
    "\n",
    "    # Log additional metrics to Azure ML\n",
    "    run.log(\"Train F1 Score\", train_f1)\n",
    "\n",
    "    # Save predictions\n",
    "    pd.DataFrame(y_train_pred, columns=[\"predictions\"]).to_csv(args.y_train_pred, index=False)\n",
    "    pd.DataFrame(y_test_pred, columns=[\"predictions\"]).to_csv(args.y_test_pred, index=False)\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--X_train_resampled\", type=str, required=True, help=\"Path to the resampled X_train data\")\n",
    "    parser.add_argument(\"--y_train_resampled\", type=str, required=True, help=\"Path to the resampled y_train data\")\n",
    "    parser.add_argument(\"--X_test\", type=str, required=True, help=\"Path to the X_test data\")\n",
    "    parser.add_argument(\"--X_train\", type=str, required=True, help=\"Path to the X_train data\")\n",
    "    parser.add_argument(\"--y_train_pred\", type=str, required=True, help=\"Path to save y_train predictions\")\n",
    "    parser.add_argument(\"--y_test_pred\", type=str, required=True, help=\"Path to save y_test predictions\")\n",
    "    return parser.parse_args()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    args = parse_args()\n",
    "    main(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/evaluation_component.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $script_folder/evaluation_component.py\n",
    "import argparse\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "from azureml.core.run import Run\n",
    "\n",
    "def main(args):\n",
    "    run = Run.get_context()  # Get Azure ML run context\n",
    "\n",
    "    # Load the actual and predicted values\n",
    "    y_train = pd.read_csv(args.y_train_actual).values.ravel()  # Flattening if necessary\n",
    "    y_train_pred = pd.read_csv(args.y_train_pred).values.ravel()  # Flattening if necessary\n",
    "    y_test = pd.read_csv(args.y_test_actual).values.ravel()  # Flattening if necessary\n",
    "    y_test_pred = pd.read_csv(args.y_test_pred).values.ravel()  # Flattening if necessary\n",
    "\n",
    "    # Calculate metrics for training set\n",
    "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "    train_precision = precision_score(y_train, y_train_pred, average='weighted')\n",
    "    train_recall = recall_score(y_train, y_train_pred, average='weighted')\n",
    "    train_f1 = f1_score(y_train, y_train_pred, average='weighted')\n",
    "\n",
    "    # Log training metrics\n",
    "    run.log(\"Train Accuracy\", train_accuracy)\n",
    "    run.log(\"Train Precision\", train_precision)\n",
    "    run.log(\"Train Recall\", train_recall)\n",
    "    run.log(\"Train F1 Score\", train_f1)\n",
    "\n",
    "    # Calculate metrics for test set\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    test_precision = precision_score(y_test, y_test_pred, average='weighted')\n",
    "    test_recall = recall_score(y_test, y_test_pred, average='weighted')\n",
    "    test_f1 = f1_score(y_test, y_test_pred, average='weighted')\n",
    "\n",
    "    # Log test metrics\n",
    "    run.log(\"Test Accuracy\", test_accuracy)\n",
    "    run.log(\"Test Precision\", test_precision)\n",
    "    run.log(\"Test Recall\", test_recall)\n",
    "    run.log(\"Test F1 Score\", test_f1)\n",
    "\n",
    "    # Print the metrics\n",
    "    print(f\"Training Metrics: Accuracy={train_accuracy}, Precision={train_precision}, Recall={train_recall}, F1 Score={train_f1}\")\n",
    "    print(f\"Testing Metrics: Accuracy={test_accuracy}, Precision={test_precision}, Recall={test_recall}, F1 Score={test_f1}\")\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--y_train_actual\", type=str, required=True, help=\"Path to the actual y_train data\")\n",
    "    parser.add_argument(\"--y_train_pred\", type=str, required=True, help=\"Path to the predicted y_train data\")\n",
    "    parser.add_argument(\"--y_test_actual\", type=str, required=True, help=\"Path to the actual y_test data\")\n",
    "    parser.add_argument(\"--y_test_pred\", type=str, required=True, help=\"Path to the predicted y_test data\")\n",
    "    return parser.parse_args()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    args = parse_args()\n",
    "    main(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1727242698635
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting load_data.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile load_data.yml\n",
    "$schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json\n",
    "name: load_data\n",
    "display_name: Load Data\n",
    "version: 1\n",
    "type: command\n",
    "inputs:\n",
    "  input_data:\n",
    "    type: uri_file\n",
    "outputs:\n",
    "  output_data:\n",
    "    type: uri_folder\n",
    "code: ./src\n",
    "environment: azureml:custom-azureml-sklearn-env@latest\n",
    "command: >\n",
    "  python load_data.py --input_data ${{inputs.input_data}} --output_data ${{outputs.output_data}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting clean_data.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile clean_data.yml\n",
    "$schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json\n",
    "name: clean_data\n",
    "display_name: Clean Missing AQI Data\n",
    "version: 1\n",
    "type: command\n",
    "inputs:\n",
    "  input_data:\n",
    "    type: uri_file\n",
    "outputs:\n",
    "  output_data:\n",
    "    type: uri_folder\n",
    "code: ./src\n",
    "environment: azureml:custom-azureml-sklearn-env@latest\n",
    "command: >\n",
    "  python clean_data.py --input_data ${{inputs.input_data}} --output_data ${{outputs.output_data}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting impute_data.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile impute_data.yml\n",
    "$schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json\n",
    "name: impute_data\n",
    "display_name: Impute Missing Values\n",
    "version: 1\n",
    "type: command\n",
    "inputs:\n",
    "  input_data:\n",
    "    type: uri_file\n",
    "outputs:\n",
    "  output_data:\n",
    "    type: uri_folder\n",
    "code: ./src\n",
    "environment: azureml:custom-azureml-sklearn-env@latest\n",
    "command: >\n",
    "  python impute_data.py --input_data ${{inputs.input_data}} --output_data ${{outputs.output_data}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting handle_outliers.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile handle_outliers.yml\n",
    "$schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json\n",
    "name: handle_outliers\n",
    "display_name: Detect and Replace Outliers\n",
    "version: 1\n",
    "type: command\n",
    "inputs:\n",
    "  input_data:\n",
    "    type: uri_file\n",
    "outputs:\n",
    "  output_data:\n",
    "    type: uri_folder\n",
    "code: ./src\n",
    "environment: azureml:custom-azureml-sklearn-env@latest\n",
    "command: >\n",
    "    python handle_outliers.py\n",
    "    --input_data ${{ inputs.input_data }}\n",
    "    --output_data ${{ outputs.output_data }}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1727251225524
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting train_test_split_component.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile train_test_split_component.yml\n",
    "$schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json\n",
    "name: train_test_split\n",
    "display_name: Train Test Split Component\n",
    "version: 1\n",
    "type: command\n",
    "inputs:\n",
    "  input_data:\n",
    "    type: uri_file\n",
    "outputs:\n",
    "  X_train_data:\n",
    "    type: uri_file\n",
    "  X_test_data:\n",
    "    type: uri_file\n",
    "  y_train_data:\n",
    "    type: uri_file\n",
    "  y_test_data:\n",
    "    type: uri_file\n",
    "code: ./src\n",
    "environment: azureml:custom-azureml-sklearn-env@latest\n",
    "command: >-\n",
    "  python train_test_split_component.py \n",
    "  --input_data ${{inputs.input_data}} \n",
    "  --X_train_data ${{outputs.X_train_data}} \n",
    "  --X_test_data ${{outputs.X_test_data}} \n",
    "  --y_train_data ${{outputs.y_train_data}} \n",
    "  --y_test_data ${{outputs.y_test_data}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting apply_smote_component.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile apply_smote_component.yml\n",
    "$schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json\n",
    "name: apply_smote\n",
    "display_name: Apply SMOTE Component\n",
    "version: 1\n",
    "type: command\n",
    "inputs:\n",
    "  X_train_data:\n",
    "    type: uri_file\n",
    "  y_train_data:\n",
    "    type: uri_file\n",
    "outputs:\n",
    "  X_resampled_data:\n",
    "    type: uri_file\n",
    "  y_resampled_data:\n",
    "    type: uri_file\n",
    "code: ./src\n",
    "environment: azureml:custom-azureml-sklearn-env@latest\n",
    "command: >-\n",
    "  python apply_smote.py \n",
    "  --X_train_data ${{inputs.X_train_data}} \n",
    "  --y_train_data ${{inputs.y_train_data}} \n",
    "  --X_resampled_data ${{outputs.X_resampled_data}} \n",
    "  --y_resampled_data ${{outputs.y_resampled_data}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1727331339395
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting modeling.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile modeling.yml\n",
    "$schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json\n",
    "name: modeling_component\n",
    "display_name: Modeling Component\n",
    "version: 1\n",
    "type: command\n",
    "inputs:\n",
    "  X_train_resampled:\n",
    "    type: uri_file\n",
    "  y_train_resampled:\n",
    "    type: uri_file\n",
    "  X_test:\n",
    "    type: uri_file\n",
    "  X_train:\n",
    "    type: uri_file\n",
    "outputs:\n",
    "  y_train_pred:\n",
    "    type: uri_file\n",
    "  y_test_pred:\n",
    "    type: uri_file\n",
    "code: ./src\n",
    "environment: azureml:custom-azureml-sklearn-env@latest\n",
    "command: >\n",
    "  python modeling_component.py \n",
    "  --X_train_resampled ${{inputs.X_train_resampled}} \n",
    "  --y_train_resampled ${{inputs.y_train_resampled}} \n",
    "  --X_test ${{inputs.X_test}} \n",
    "  --X_train ${{inputs.X_train}} \n",
    "  --y_train_pred ${{outputs.y_train_pred}} \n",
    "  --y_test_pred ${{outputs.y_test_pred}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1727332203516
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting evaluation_component.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile evaluation_component.yml\n",
    "$schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json\n",
    "name: evaluate_model\n",
    "display_name: Model Evaluation Component\n",
    "version: 1\n",
    "type: command\n",
    "inputs:\n",
    "  y_train_actual:\n",
    "    type: uri_file\n",
    "  y_train_pred:\n",
    "    type: uri_file\n",
    "  y_test_actual:\n",
    "    type: uri_file\n",
    "  y_test_pred:\n",
    "    type: uri_file\n",
    "# outputs:\n",
    "#   evaluation_results:\n",
    "#     type: uri_file\n",
    "code: ./src\n",
    "environment: azureml:custom-azureml-sklearn-env@latest\n",
    "command: >-\n",
    "  python evaluation_component.py \n",
    "  --y_train_actual ${{inputs.y_train_actual}} \n",
    "  --y_train_pred ${{inputs.y_train_pred}} \n",
    "  --y_test_actual ${{inputs.y_test_actual}} \n",
    "  --y_test_pred ${{inputs.y_test_pred}}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Components in Azure ML\n",
    "In Azure Machine Learning, **components** are reusable, modular building blocks that represent specific tasks within a machine learning pipeline. These tasks can range from data loading and preprocessing to model training, evaluation, or even custom operations. Components are typically implemented as Python scripts or executable tasks, with clearly defined input and output interfaces.\n",
    "\n",
    "Each component operates independently and can be customized with parameters, making them versatile and adaptable to different projects. The modular nature of components enables them to be easily reused across multiple pipelines, allowing data scientists to build workflows efficiently by combining and orchestrating these components as needed. This modularity and reusability not only streamline the machine learning workflow but also improve collaboration, scalability, and resource optimization..\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1727419416974
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from azure.ai.ml import load_component\n",
    "\n",
    "# Define the directory where your YAML files are stored\n",
    "parent_dir = \"./\"  # Change this path as needed\n",
    "\n",
    "# Load the components using load_component\n",
    "load_data_component = load_component(source=parent_dir + \"load_data.yml\")\n",
    "clean_data_component = load_component(source=parent_dir + \"clean_data.yml\")\n",
    "impute_data_component = load_component(source=parent_dir + \"impute_data.yml\")\n",
    "handle_outliers_component = load_component(source=parent_dir + \"handle_outliers.yml\")\n",
    "train_test_split_component = load_component(source=parent_dir + \"train_test_split_component.yml\")\n",
    "smote_component = load_component(source=parent_dir + \"apply_smote_component.yml\")\n",
    "modeling_component = load_component(source=parent_dir + \"modeling.yml\")\n",
    "evaluation_component = load_component(source=parent_dir + \"evaluation_component.yml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Azure ML Pipeline\n",
    "An **Azure Machine Learning pipeline** orchestrates a series of steps (components) to automate the machine learning workflow. Pipelines can include data ingestion, preprocessing, model training, hyperparameter tuning, and model evaluation. Each step in the pipeline can be run on different compute resources, allowing for parallelism and efficient resource utilization.\n",
    "\n",
    "In this project, the pipeline is designed to manage multiple tasks like handling data transformation using custom scripts, applying models, and evaluating results in a sequence to ensure the complete ML lifecycle is automated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1727419417285
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from azure.ai.ml import Input\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "from azure.ai.ml.dsl import pipeline\n",
    "\n",
    "@pipeline()\n",
    "def AQI_classification(pipeline_job_input):\n",
    "    load_data = load_data_component(input_data=pipeline_job_input)\n",
    "    clean_data = clean_data_component(input_data=load_data.outputs.output_data)\n",
    "    impute_data = impute_data_component(input_data=clean_data.outputs.output_data)\n",
    "    handle_outliers = handle_outliers_component(input_data=impute_data.outputs.output_data)\n",
    "    train_test_split_data = train_test_split_component(input_data=handle_outliers.outputs.output_data)\n",
    "    smote = smote_component( X_train_data=train_test_split_data.outputs.X_train_data, \n",
    "        y_train_data=train_test_split_data.outputs.y_train_data)\n",
    "    modeling = modeling_component(X_train_resampled = smote.outputs.X_resampled_data, y_train_resampled = smote.outputs.y_resampled_data, \n",
    "  X_test = train_test_split_data.outputs.X_test_data,X_train = train_test_split_data.outputs.X_train_data)\n",
    "    evaluation = evaluation_component(y_train_actual = train_test_split_data.outputs.y_train_data ,\n",
    "  y_train_pred= modeling.outputs.y_train_pred ,\n",
    "  y_test_actual=train_test_split_data.outputs.y_test_data , \n",
    "  y_test_pred = modeling.outputs.y_test_pred)\n",
    "    return {\n",
    "         \"y_test_actual\" : evaluation.outputs.y_test_actual,\n",
    "        \"y_test_pred\" : evaluation.outputs.y_test_pred,\n",
    "     }\n",
    "\n",
    "pipeline_job = AQI_classification(Input(type=AssetTypes.URI_FILE, path=\"azureml://subscriptions/edc61183-6780-4b6c-8c4a-a5fb6ae6b788/resourcegroups/RaghavaVigneshwar/workspaces/raghava/datastores/workspaceblobstore/paths/UI/2024-09-27_053645_UTC/city_day.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1727419417579
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Class AutoDeleteSettingSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class AutoDeleteConditionSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class BaseAutoDeleteSettingSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class IntellectualPropertySchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class ProtectionLevelSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class BaseIntellectualPropertySchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "display_name: AQI_classification\n",
      "type: pipeline\n",
      "inputs:\n",
      "  pipeline_job_input:\n",
      "    type: uri_file\n",
      "    path: azureml://subscriptions/edc61183-6780-4b6c-8c4a-a5fb6ae6b788/resourcegroups/RaghavaVigneshwar/workspaces/raghava/datastores/workspaceblobstore/paths/UI/2024-09-27_053645_UTC/city_day.csv\n",
      "jobs:\n",
      "  load_data:\n",
      "    type: command\n",
      "    inputs:\n",
      "      input_data:\n",
      "        path: ${{parent.inputs.pipeline_job_input}}\n",
      "    component:\n",
      "      $schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json\n",
      "      name: load_data\n",
      "      version: '1'\n",
      "      display_name: Load Data\n",
      "      type: command\n",
      "      inputs:\n",
      "        input_data:\n",
      "          type: uri_file\n",
      "      outputs:\n",
      "        output_data:\n",
      "          type: uri_folder\n",
      "      command: 'python load_data.py --input_data ${{inputs.input_data}} --output_data\n",
      "        ${{outputs.output_data}}\n",
      "\n",
      "        '\n",
      "      environment: azureml:custom-azureml-sklearn-env@latest\n",
      "      code: /mnt/batch/tasks/shared/LS_root/mounts/clusters/raghava/code/Users/User1-44320065/src\n",
      "      is_deterministic: true\n",
      "  clean_data:\n",
      "    type: command\n",
      "    inputs:\n",
      "      input_data:\n",
      "        path: ${{parent.jobs.load_data.outputs.output_data}}\n",
      "    component:\n",
      "      $schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json\n",
      "      name: clean_data\n",
      "      version: '1'\n",
      "      display_name: Clean Missing AQI Data\n",
      "      type: command\n",
      "      inputs:\n",
      "        input_data:\n",
      "          type: uri_file\n",
      "      outputs:\n",
      "        output_data:\n",
      "          type: uri_folder\n",
      "      command: 'python clean_data.py --input_data ${{inputs.input_data}} --output_data\n",
      "        ${{outputs.output_data}}\n",
      "\n",
      "        '\n",
      "      environment: azureml:custom-azureml-sklearn-env@latest\n",
      "      code: /mnt/batch/tasks/shared/LS_root/mounts/clusters/raghava/code/Users/User1-44320065/src\n",
      "      is_deterministic: true\n",
      "  impute_data:\n",
      "    type: command\n",
      "    inputs:\n",
      "      input_data:\n",
      "        path: ${{parent.jobs.clean_data.outputs.output_data}}\n",
      "    component:\n",
      "      $schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json\n",
      "      name: impute_data\n",
      "      version: '1'\n",
      "      display_name: Impute Missing Values\n",
      "      type: command\n",
      "      inputs:\n",
      "        input_data:\n",
      "          type: uri_file\n",
      "      outputs:\n",
      "        output_data:\n",
      "          type: uri_folder\n",
      "      command: 'python impute_data.py --input_data ${{inputs.input_data}} --output_data\n",
      "        ${{outputs.output_data}}\n",
      "\n",
      "        '\n",
      "      environment: azureml:custom-azureml-sklearn-env@latest\n",
      "      code: /mnt/batch/tasks/shared/LS_root/mounts/clusters/raghava/code/Users/User1-44320065/src\n",
      "      is_deterministic: true\n",
      "  handle_outliers:\n",
      "    type: command\n",
      "    inputs:\n",
      "      input_data:\n",
      "        path: ${{parent.jobs.impute_data.outputs.output_data}}\n",
      "    component:\n",
      "      $schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json\n",
      "      name: handle_outliers\n",
      "      version: '1'\n",
      "      display_name: Detect and Replace Outliers\n",
      "      type: command\n",
      "      inputs:\n",
      "        input_data:\n",
      "          type: uri_file\n",
      "      outputs:\n",
      "        output_data:\n",
      "          type: uri_folder\n",
      "      command: 'python handle_outliers.py --input_data ${{ inputs.input_data }} --output_data\n",
      "        ${{ outputs.output_data }}\n",
      "\n",
      "        '\n",
      "      environment: azureml:custom-azureml-sklearn-env@latest\n",
      "      code: /mnt/batch/tasks/shared/LS_root/mounts/clusters/raghava/code/Users/User1-44320065/src\n",
      "      is_deterministic: true\n",
      "  train_test_split_data:\n",
      "    type: command\n",
      "    inputs:\n",
      "      input_data:\n",
      "        path: ${{parent.jobs.handle_outliers.outputs.output_data}}\n",
      "    component:\n",
      "      $schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json\n",
      "      name: train_test_split\n",
      "      version: '1'\n",
      "      display_name: Train Test Split Component\n",
      "      type: command\n",
      "      inputs:\n",
      "        input_data:\n",
      "          type: uri_file\n",
      "      outputs:\n",
      "        X_train_data:\n",
      "          type: uri_file\n",
      "        X_test_data:\n",
      "          type: uri_file\n",
      "        y_train_data:\n",
      "          type: uri_file\n",
      "        y_test_data:\n",
      "          type: uri_file\n",
      "      command: python train_test_split_component.py  --input_data ${{inputs.input_data}}  --X_train_data\n",
      "        ${{outputs.X_train_data}}  --X_test_data ${{outputs.X_test_data}}  --y_train_data\n",
      "        ${{outputs.y_train_data}}  --y_test_data ${{outputs.y_test_data}}\n",
      "      environment: azureml:custom-azureml-sklearn-env@latest\n",
      "      code: /mnt/batch/tasks/shared/LS_root/mounts/clusters/raghava/code/Users/User1-44320065/src\n",
      "      is_deterministic: true\n",
      "  smote:\n",
      "    type: command\n",
      "    inputs:\n",
      "      X_train_data:\n",
      "        path: ${{parent.jobs.train_test_split_data.outputs.X_train_data}}\n",
      "      y_train_data:\n",
      "        path: ${{parent.jobs.train_test_split_data.outputs.y_train_data}}\n",
      "    component:\n",
      "      $schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json\n",
      "      name: apply_smote\n",
      "      version: '1'\n",
      "      display_name: Apply SMOTE Component\n",
      "      type: command\n",
      "      inputs:\n",
      "        X_train_data:\n",
      "          type: uri_file\n",
      "        y_train_data:\n",
      "          type: uri_file\n",
      "      outputs:\n",
      "        X_resampled_data:\n",
      "          type: uri_file\n",
      "        y_resampled_data:\n",
      "          type: uri_file\n",
      "      command: python apply_smote.py  --X_train_data ${{inputs.X_train_data}}  --y_train_data\n",
      "        ${{inputs.y_train_data}}  --X_resampled_data ${{outputs.X_resampled_data}}  --y_resampled_data\n",
      "        ${{outputs.y_resampled_data}}\n",
      "      environment: azureml:custom-azureml-sklearn-env@latest\n",
      "      code: /mnt/batch/tasks/shared/LS_root/mounts/clusters/raghava/code/Users/User1-44320065/src\n",
      "      is_deterministic: true\n",
      "  modeling:\n",
      "    type: command\n",
      "    inputs:\n",
      "      X_train_resampled:\n",
      "        path: ${{parent.jobs.smote.outputs.X_resampled_data}}\n",
      "      y_train_resampled:\n",
      "        path: ${{parent.jobs.smote.outputs.y_resampled_data}}\n",
      "      X_test:\n",
      "        path: ${{parent.jobs.train_test_split_data.outputs.X_test_data}}\n",
      "      X_train:\n",
      "        path: ${{parent.jobs.train_test_split_data.outputs.X_train_data}}\n",
      "    component:\n",
      "      $schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json\n",
      "      name: modeling_component\n",
      "      version: '1'\n",
      "      display_name: Modeling Component\n",
      "      type: command\n",
      "      inputs:\n",
      "        X_train_resampled:\n",
      "          type: uri_file\n",
      "        y_train_resampled:\n",
      "          type: uri_file\n",
      "        X_test:\n",
      "          type: uri_file\n",
      "        X_train:\n",
      "          type: uri_file\n",
      "      outputs:\n",
      "        y_train_pred:\n",
      "          type: uri_file\n",
      "        y_test_pred:\n",
      "          type: uri_file\n",
      "      command: 'python modeling_component.py  --X_train_resampled ${{inputs.X_train_resampled}}  --y_train_resampled\n",
      "        ${{inputs.y_train_resampled}}  --X_test ${{inputs.X_test}}  --X_train ${{inputs.X_train}}  --y_train_pred\n",
      "        ${{outputs.y_train_pred}}  --y_test_pred ${{outputs.y_test_pred}}\n",
      "\n",
      "        '\n",
      "      environment: azureml:custom-azureml-sklearn-env@latest\n",
      "      code: /mnt/batch/tasks/shared/LS_root/mounts/clusters/raghava/code/Users/User1-44320065/src\n",
      "      is_deterministic: true\n",
      "  evaluation:\n",
      "    type: command\n",
      "    inputs:\n",
      "      y_train_actual:\n",
      "        path: ${{parent.jobs.train_test_split_data.outputs.y_train_data}}\n",
      "      y_train_pred:\n",
      "        path: ${{parent.jobs.modeling.outputs.y_train_pred}}\n",
      "      y_test_actual:\n",
      "        path: ${{parent.jobs.train_test_split_data.outputs.y_test_data}}\n",
      "      y_test_pred:\n",
      "        path: ${{parent.jobs.modeling.outputs.y_test_pred}}\n",
      "    component:\n",
      "      $schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json\n",
      "      name: evaluate_model\n",
      "      version: '1'\n",
      "      display_name: Model Evaluation Component\n",
      "      type: command\n",
      "      inputs:\n",
      "        y_train_actual:\n",
      "          type: uri_file\n",
      "        y_train_pred:\n",
      "          type: uri_file\n",
      "        y_test_actual:\n",
      "          type: uri_file\n",
      "        y_test_pred:\n",
      "          type: uri_file\n",
      "      command: python evaluation_component.py  --y_train_actual ${{inputs.y_train_actual}}  --y_train_pred\n",
      "        ${{inputs.y_train_pred}}  --y_test_actual ${{inputs.y_test_actual}}  --y_test_pred\n",
      "        ${{inputs.y_test_pred}}\n",
      "      environment: azureml:custom-azureml-sklearn-env@latest\n",
      "      code: /mnt/batch/tasks/shared/LS_root/mounts/clusters/raghava/code/Users/User1-44320065/src\n",
      "      is_deterministic: true\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(pipeline_job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1727419417883
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "display_name: AQI_classification\n",
      "type: pipeline\n",
      "inputs:\n",
      "  pipeline_job_input:\n",
      "    type: uri_file\n",
      "    path: azureml://subscriptions/edc61183-6780-4b6c-8c4a-a5fb6ae6b788/resourcegroups/RaghavaVigneshwar/workspaces/raghava/datastores/workspaceblobstore/paths/UI/2024-09-27_053645_UTC/city_day.csv\n",
      "jobs:\n",
      "  load_data:\n",
      "    type: command\n",
      "    inputs:\n",
      "      input_data:\n",
      "        path: ${{parent.inputs.pipeline_job_input}}\n",
      "    component:\n",
      "      $schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json\n",
      "      name: load_data\n",
      "      version: '1'\n",
      "      display_name: Load Data\n",
      "      type: command\n",
      "      inputs:\n",
      "        input_data:\n",
      "          type: uri_file\n",
      "      outputs:\n",
      "        output_data:\n",
      "          type: uri_folder\n",
      "      command: 'python load_data.py --input_data ${{inputs.input_data}} --output_data\n",
      "        ${{outputs.output_data}}\n",
      "\n",
      "        '\n",
      "      environment: azureml:custom-azureml-sklearn-env@latest\n",
      "      code: /mnt/batch/tasks/shared/LS_root/mounts/clusters/raghava/code/Users/User1-44320065/src\n",
      "      is_deterministic: true\n",
      "  clean_data:\n",
      "    type: command\n",
      "    inputs:\n",
      "      input_data:\n",
      "        path: ${{parent.jobs.load_data.outputs.output_data}}\n",
      "    component:\n",
      "      $schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json\n",
      "      name: clean_data\n",
      "      version: '1'\n",
      "      display_name: Clean Missing AQI Data\n",
      "      type: command\n",
      "      inputs:\n",
      "        input_data:\n",
      "          type: uri_file\n",
      "      outputs:\n",
      "        output_data:\n",
      "          type: uri_folder\n",
      "      command: 'python clean_data.py --input_data ${{inputs.input_data}} --output_data\n",
      "        ${{outputs.output_data}}\n",
      "\n",
      "        '\n",
      "      environment: azureml:custom-azureml-sklearn-env@latest\n",
      "      code: /mnt/batch/tasks/shared/LS_root/mounts/clusters/raghava/code/Users/User1-44320065/src\n",
      "      is_deterministic: true\n",
      "  impute_data:\n",
      "    type: command\n",
      "    inputs:\n",
      "      input_data:\n",
      "        path: ${{parent.jobs.clean_data.outputs.output_data}}\n",
      "    component:\n",
      "      $schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json\n",
      "      name: impute_data\n",
      "      version: '1'\n",
      "      display_name: Impute Missing Values\n",
      "      type: command\n",
      "      inputs:\n",
      "        input_data:\n",
      "          type: uri_file\n",
      "      outputs:\n",
      "        output_data:\n",
      "          type: uri_folder\n",
      "      command: 'python impute_data.py --input_data ${{inputs.input_data}} --output_data\n",
      "        ${{outputs.output_data}}\n",
      "\n",
      "        '\n",
      "      environment: azureml:custom-azureml-sklearn-env@latest\n",
      "      code: /mnt/batch/tasks/shared/LS_root/mounts/clusters/raghava/code/Users/User1-44320065/src\n",
      "      is_deterministic: true\n",
      "  handle_outliers:\n",
      "    type: command\n",
      "    inputs:\n",
      "      input_data:\n",
      "        path: ${{parent.jobs.impute_data.outputs.output_data}}\n",
      "    component:\n",
      "      $schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json\n",
      "      name: handle_outliers\n",
      "      version: '1'\n",
      "      display_name: Detect and Replace Outliers\n",
      "      type: command\n",
      "      inputs:\n",
      "        input_data:\n",
      "          type: uri_file\n",
      "      outputs:\n",
      "        output_data:\n",
      "          type: uri_folder\n",
      "      command: 'python handle_outliers.py --input_data ${{ inputs.input_data }} --output_data\n",
      "        ${{ outputs.output_data }}\n",
      "\n",
      "        '\n",
      "      environment: azureml:custom-azureml-sklearn-env@latest\n",
      "      code: /mnt/batch/tasks/shared/LS_root/mounts/clusters/raghava/code/Users/User1-44320065/src\n",
      "      is_deterministic: true\n",
      "  train_test_split_data:\n",
      "    type: command\n",
      "    inputs:\n",
      "      input_data:\n",
      "        path: ${{parent.jobs.handle_outliers.outputs.output_data}}\n",
      "    component:\n",
      "      $schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json\n",
      "      name: train_test_split\n",
      "      version: '1'\n",
      "      display_name: Train Test Split Component\n",
      "      type: command\n",
      "      inputs:\n",
      "        input_data:\n",
      "          type: uri_file\n",
      "      outputs:\n",
      "        X_train_data:\n",
      "          type: uri_file\n",
      "        X_test_data:\n",
      "          type: uri_file\n",
      "        y_train_data:\n",
      "          type: uri_file\n",
      "        y_test_data:\n",
      "          type: uri_file\n",
      "      command: python train_test_split_component.py  --input_data ${{inputs.input_data}}  --X_train_data\n",
      "        ${{outputs.X_train_data}}  --X_test_data ${{outputs.X_test_data}}  --y_train_data\n",
      "        ${{outputs.y_train_data}}  --y_test_data ${{outputs.y_test_data}}\n",
      "      environment: azureml:custom-azureml-sklearn-env@latest\n",
      "      code: /mnt/batch/tasks/shared/LS_root/mounts/clusters/raghava/code/Users/User1-44320065/src\n",
      "      is_deterministic: true\n",
      "  smote:\n",
      "    type: command\n",
      "    inputs:\n",
      "      X_train_data:\n",
      "        path: ${{parent.jobs.train_test_split_data.outputs.X_train_data}}\n",
      "      y_train_data:\n",
      "        path: ${{parent.jobs.train_test_split_data.outputs.y_train_data}}\n",
      "    component:\n",
      "      $schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json\n",
      "      name: apply_smote\n",
      "      version: '1'\n",
      "      display_name: Apply SMOTE Component\n",
      "      type: command\n",
      "      inputs:\n",
      "        X_train_data:\n",
      "          type: uri_file\n",
      "        y_train_data:\n",
      "          type: uri_file\n",
      "      outputs:\n",
      "        X_resampled_data:\n",
      "          type: uri_file\n",
      "        y_resampled_data:\n",
      "          type: uri_file\n",
      "      command: python apply_smote.py  --X_train_data ${{inputs.X_train_data}}  --y_train_data\n",
      "        ${{inputs.y_train_data}}  --X_resampled_data ${{outputs.X_resampled_data}}  --y_resampled_data\n",
      "        ${{outputs.y_resampled_data}}\n",
      "      environment: azureml:custom-azureml-sklearn-env@latest\n",
      "      code: /mnt/batch/tasks/shared/LS_root/mounts/clusters/raghava/code/Users/User1-44320065/src\n",
      "      is_deterministic: true\n",
      "  modeling:\n",
      "    type: command\n",
      "    inputs:\n",
      "      X_train_resampled:\n",
      "        path: ${{parent.jobs.smote.outputs.X_resampled_data}}\n",
      "      y_train_resampled:\n",
      "        path: ${{parent.jobs.smote.outputs.y_resampled_data}}\n",
      "      X_test:\n",
      "        path: ${{parent.jobs.train_test_split_data.outputs.X_test_data}}\n",
      "      X_train:\n",
      "        path: ${{parent.jobs.train_test_split_data.outputs.X_train_data}}\n",
      "    component:\n",
      "      $schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json\n",
      "      name: modeling_component\n",
      "      version: '1'\n",
      "      display_name: Modeling Component\n",
      "      type: command\n",
      "      inputs:\n",
      "        X_train_resampled:\n",
      "          type: uri_file\n",
      "        y_train_resampled:\n",
      "          type: uri_file\n",
      "        X_test:\n",
      "          type: uri_file\n",
      "        X_train:\n",
      "          type: uri_file\n",
      "      outputs:\n",
      "        y_train_pred:\n",
      "          type: uri_file\n",
      "        y_test_pred:\n",
      "          type: uri_file\n",
      "      command: 'python modeling_component.py  --X_train_resampled ${{inputs.X_train_resampled}}  --y_train_resampled\n",
      "        ${{inputs.y_train_resampled}}  --X_test ${{inputs.X_test}}  --X_train ${{inputs.X_train}}  --y_train_pred\n",
      "        ${{outputs.y_train_pred}}  --y_test_pred ${{outputs.y_test_pred}}\n",
      "\n",
      "        '\n",
      "      environment: azureml:custom-azureml-sklearn-env@latest\n",
      "      code: /mnt/batch/tasks/shared/LS_root/mounts/clusters/raghava/code/Users/User1-44320065/src\n",
      "      is_deterministic: true\n",
      "  evaluation:\n",
      "    type: command\n",
      "    inputs:\n",
      "      y_train_actual:\n",
      "        path: ${{parent.jobs.train_test_split_data.outputs.y_train_data}}\n",
      "      y_train_pred:\n",
      "        path: ${{parent.jobs.modeling.outputs.y_train_pred}}\n",
      "      y_test_actual:\n",
      "        path: ${{parent.jobs.train_test_split_data.outputs.y_test_data}}\n",
      "      y_test_pred:\n",
      "        path: ${{parent.jobs.modeling.outputs.y_test_pred}}\n",
      "    component:\n",
      "      $schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json\n",
      "      name: evaluate_model\n",
      "      version: '1'\n",
      "      display_name: Model Evaluation Component\n",
      "      type: command\n",
      "      inputs:\n",
      "        y_train_actual:\n",
      "          type: uri_file\n",
      "        y_train_pred:\n",
      "          type: uri_file\n",
      "        y_test_actual:\n",
      "          type: uri_file\n",
      "        y_test_pred:\n",
      "          type: uri_file\n",
      "      command: python evaluation_component.py  --y_train_actual ${{inputs.y_train_actual}}  --y_train_pred\n",
      "        ${{inputs.y_train_pred}}  --y_test_actual ${{inputs.y_test_actual}}  --y_test_pred\n",
      "        ${{inputs.y_test_pred}}\n",
      "      environment: azureml:custom-azureml-sklearn-env@latest\n",
      "      code: /mnt/batch/tasks/shared/LS_root/mounts/clusters/raghava/code/Users/User1-44320065/src\n",
      "      is_deterministic: true\n",
      "settings:\n",
      "  default_datastore: azureml:workspaceblobstore\n",
      "  default_compute: azureml:CLUSTER1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pipeline_job.settings.default_compute = \"CLUSTER1\"\n",
    "pipeline_job.settings.default_datastore = \"workspaceblobstore\"\n",
    "print(pipeline_job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1727419436907
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[32mUploading src (0.02 MBs):   0%|          | 0/16432 [00:00<?, ?it/s]\r",
      "\u001b[32mUploading src (0.02 MBs): 100%|██████████| 16432/16432 [00:00<00:00, 152339.78it/s]\r",
      "\u001b[32mUploading src (0.02 MBs): 100%|██████████| 16432/16432 [00:00<00:00, 150663.03it/s]\n",
      "\u001b[39m\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monitor your pipeline_job at https://ml.azure.com/runs/boring_leg_r7dvr0xmjz?wsid=/subscriptions/edc61183-6780-4b6c-8c4a-a5fb6ae6b788/resourcegroups/raghavavigneshwar/workspaces/raghava&tid=4cfe372a-37a4-44f8-91b2-5faf34253c62\n"
     ]
    }
   ],
   "source": [
    "# submit job to workspace\n",
    "pipeline_job = ml_client.jobs.create_or_update(\n",
    "    pipeline_job, experiment_name=\"pipeline_AQI1\"\n",
    ")\n",
    "\n",
    "aml_url = pipeline_job.studio_url\n",
    "print(\"Monitor your pipeline_job at\", aml_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python38-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   },
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
